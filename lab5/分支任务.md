# 分支任务
## Lab2：gdb 调试页表查询过程
### 有些模型的表现确实一言难尽
一开始，我们在vscode中使用copilot，但没有使用高级模型，感觉不用写代码的话随便用个模型就行，但GPT5-mini的效果简直太差了，问了好几个问题硬是没给我产生一个合适的调试流程，输出的内容也乱七八糟。所以我们改用Gemini3 Pro辅助完成这个任务。
### “根据实验指导，看看怎么完成这个调试任务”
我们先问了这样一个笼统的问题，看看大模型自己读文档的能力怎么样。根据提示，在ucore调试的终端打上kern_init的断点，然后输入continue执行到这个断点，接着开启qemu调试的终端，在riscv_cpu_tlb_fill这个入口打上断点，这样当ucore执行到访存指令，就可以看到在这个函数触发断点了：
```c
Thread 2 "qemu-system-ris" hit Breakpoint 1, riscv_cpu_tlb_fill (
    cs=0x572aa0c06630, 
    address=18446744072637923320, size=8, 
    access_type=MMU_DATA_STORE, 
    mmu_idx=1, probe=false, 
    retaddr=137148875407731)
    at /home/alanliu/qemu/qemu-4.1.1/target/riscv/cpu_helper.c:438
```
此时最大的问题是地址以十进制显示，我们先不管这件事。通过单步执行，期待能看到诸如PTE读取、多级页表遍历等，但经过很多次单步执行后，并没有找到这些核心逻辑，所以我们要调整一下调试的方案。
### “现在可以调试，但没有触及页表查询过程的核心”
这时，大模型给了我另一个函数——get_physical_address，我们在这里添加断点，可能是这次没有先在ucore的终端执行到入口位置的缘故，我们似乎在qemu调试终端看到了M态的地址映射：
```c
Thread 2 "qemu-system-ris" hit Breakpoint 1, get_physical_address
    (env=0x5c1818dd5040, 
    physical=0x77ac25b7c270, 
    prot=0x77ac25b7c264, 
    addr=2147522064, 
    access_type=0, mmu_idx=3)
    at /home/alanliu/qemu/qemu-4.1.1/target/riscv/cpu_helper.c:158
158     {
(gdb) n
163         int mode = mmu_idx;
(gdb) n
165         if (mode == PRV_M && access_type != MMU_INST_FETCH) {
(gdb) 
166             if (get_field(env->mstatus, MSTATUS_MPRV)) {
(gdb) n
171         if (mode == PRV_M || !riscv_feature(env, RISCV_FEATURE_MMU)) {
(gdb) n
172             *physical = addr;
(gdb) n
173             *prot = PAGE_READ | PAGE_WRITE | PAGE_EXEC;
(gdb) n
174             return TRANSLATE_SUCCESS;
(gdb) 
353     }
```
mmu_idx=3意味着此时RISC-V处于M态，内部的代码表示绕过MMU，虚拟地址直接等于物理地址.
之后我们先让ucore执行到kern_init处，这样应该就是在S态进行地址访问了。我们执行之后，进入了虚拟地址到物理地址的转换：
```c
Thread 1 "qemu-system-ris" hit Breakpoint 1, get_physical_address
    (env=0x64c76b6a3040, 
    physical=0x7ffc6c4a8dc8, 
    prot=0x7ffc6c4a8dc0, 
    addr=18446744072637906944, 
    access_type=0, mmu_idx=1)
```
我们看到这一次mmu_idx=1，意味着我们捕获了S态的情况；addr虽然还是很难看的十进制，但转换后是0xFFFFFFFFC0001000，就是在ucore断点处输入print/x $sp得到的地址。更令人兴奋的是，我们看到了对3级页表的遍历循环的代码：
```c
 levels = 3; ptidxbits = 9; ptesize = 8; 
for (i = 0; i < levels; i++, ptshift -= ptidxbits) 
{
    ...
    target_ulong idx = (addr >> (PGSHIFT + ptshift)) &((1 << ptidxbits) - 1); 
    target_ulong idx = (addr >> (PGSHIFT + ptshift)) & target_ulong pte_addr = base + idx * ptesize;
    ....
    target_ulong pte = ldq_phys(cs->as, pte_addr);
    target_ulong ppn = pte >> PTE_PPN_SHIFT;  
    ...
    *physical = (ppn | (vpn & ((1L << ptshift) - 1))) << PGSHIFT;
}
```
其中也定义了SV39页表的一些参数: 3级，每级9位索引，64位PTE，这是不是和我们之前的认知对上了？
我们在代码里还展示了页表翻译的核心逻辑：
1. 提取当前级页表的索引（虚拟地址中对应9位）
1. 计算当前级PTE（页表项）的物理地址（PTE项的大小是8字节）
1. 读取PTE并提取PTE中的物理页号（PPN），作为下一级的base
1. 计算最终物理地址：PA=PPN<<12 + 页内偏移（最后一级翻译）
### “我现在已经了解了qemu的地址转换机制了，能不能调试出tlb的逻辑（像第一次调试那么做可以吗？第一次好像并没有看出来什么特别的逻辑）”
我们刚刚观察的实际上是TLBmiss之后的处理逻辑，QEMU的处理逻辑应该是“查TLB->TLBmiss->TLB_fill->get_physical_address。我们现在想看到的就是get_physical_address之前的流程。
首先我们在ucore什么也不做，这样捕获到的qemu行为就是开启分页之前的行为了，我们在qemu调试终端打上断点**break riscv_cpu_tlb_fill**，然后进行单步调试，重要结果如下：
```c
Thread 2 "qemu-system-ris" hit Breakpoint 1, riscv_cpu_tlb_fill (
    cs=0x56bf6834a630, 
    address=4096, size=0, 
    access_type=MMU_INST_FETCH, 
    mmu_idx=3, probe=false, 
    retaddr=0)
    {
    ...
    ret = get_physical_address(env, &pa, &prot, address, access_type, mmu_idx);
    ...
    tlb_set_page(cs, address & TARGET_PAGE_MASK, pa & TARGET_PAGE_MASK,
    }
```
我们看到的是M状态下TLB填充的逻辑，我们发现它调用了get_physical_address，直接返回物理地址。而TLB 填充的最终核心步骤是tlb_set_page，从函数调用可以看出来，大概率是建立了address（VA）到pa的映射。
之后我们直接连续跳转，直到执行S状态的 riscv_cpu_tlb_fill，他们的逻辑是一样的，最明显的不同还是用三级页表实现了虚拟地址到物理地址的映射。
其实我们还单步调试了tlb_set_page，发现里面有好多层封装，就不仔细看了......
### “我只看到了TLB填充的逻辑，但没调试查找的逻辑呀，你刚刚告诉我这很困难，但能不能提供一点方法”
大模型告诉我，刚刚调试的内容属于“慢路径”，而查找TLB属于“快路径”，他们通常被编译成了机器码。不过大模型说qemu保留了 C 语言版本的 Helper 函数 作为通用入口，所以可以调试这些Helper函数。大模型不确定tlb查找的helper函数的具体名字，所以需要确认：**info functions helper_le_ldq_mmu**，发现果然叫这个名字，在tcg/cputlb.c中。在此处大话三国断点，并执行到这里：
```c
Thread 2 "qemu-system-ris" hit Breakpoint 1, helper_le_ldq_mmu (
    env=0x598a993c6040, 
    addr=18446744072671463212, 
    oi=49, 
    retaddr=137601658979012)
    at /home/alanliu/qemu/qemu-4.1.1/accel/tcg/cputlb.c:1449
1449        return load_helper(env, addr, oi, retaddr, 8, false, false,
(gdb) n
1451    }
```
发现里面只有一个语句，想必功能都封装在load_helper中，我们启动单步调试（s），进入函数内部进行查看，发现了几行关键代码：
```c
1252        uintptr_t mmu_idx = get_mmuidx(oi);
(gdb) n
1253        uintptr_t index = tlb_index(env, mmu_idx, addr);
(gdb) n
1254        CPUTLBEntry *entry = tlb_entry(env, mmu_idx, addr);
(gdb) n
1255        target_ulong tlb_addr = code_read ? entry->addr_code : entry->addr_read; 
...
1271        if (!tlb_hit(tlb_addr, addr)) {
(gdb) n
1272            if (!victim_tlb_hit(env, mmu_idx, index, tlb_off,
(gdb) n
1274                tlb_fill(env_cpu(env), addr, size,
(gdb) n
1276                index = tlb_index(env, mmu_idx, addr);
(gdb) n
1277                entry = tlb_entry(env, mmu_idx, addr);
(gdb) n
1279            tlb_addr = code_read ? entry->addr_code : entry->addr_read;
...
(gdb) n
1338        haddr = (void *)((uintptr_t)addr + entry->addend);
```
mmu_idx是特权级索引，代表了要查哪一个特权级的TLB；index就是TLB索引，相当于数组下标，由虚拟地址的哈希计算得来；entry是TLB条目指针，指向index位置的TLB条目；tlb_addr条目中记录的虚拟地址；haddr是使用TLB查表结果计算物理地址。
当TLB未命中并且victim tlb也没命中时，就启用tlb填充，并且重新计算index和entry，确保最新。

### “handle SIGPIPE nostop noprint有什么作用”
我们暂时没有遇到这个问题，大模型的解释是这个命令让调试qemu的那个终端在收到 SIGPIPE时，GDB 不暂停、不打印，进程继续运行。调试QEMU时，可能偶尔会触发SIGPIPE，比如管道临时关闭、套接字短暂断开。
到这里，我们lab2的调试任务基本完成了。

## 分支任务：gdb 调试系统调用以及返回
有了上一个分支任务的调试经验，这一次我们的目标更清晰了，就是调试ecall和sret。ecall在syscall()中，不过要注意的是，这里的syscall()并不是内核里的那个封装，而是用户态的封装，其中实现了一段汇编语言，使用了ecall，然后进入trap处理，才使用到了内核态的那个syscall()封装。
根据实验指导，我们在调试ucore之前，要使用命令**add-symbol-file obj/__user_exit.out**来手动向 GDB 导入用户程序的符号表，否则它只加载了内核的符号表。这样我们就可以对用户程序打断点了。由于我们运行的是exit.c，并且要观察其中的系统调用fork()和exit()，所以我们打上断点**b user/exit.c:9**
我们执行到exit.c中，再打上一个断点**break user/libs/syscall.c:18**，马上就可以看到ecall了！
### “看看实验指导，告诉我qemu调试应该打上什么断点”
在执行ecall之前，还得先给qemu打上断点，这样才能捕捉到ecall执行时的信息，大模型提示我给riscv_cpu_do_interrupt打上断点，因为这个函数处理所有中断和异常，由于ucore已经执行到了ecall，所以也不必加条件断点了，应该会直接停在ecall触发的那个riscv_cpu_do_interrupt。
我们在syscall的汇编代码处停下后，通过调试qemu，看到了一些关键代码：
```c
if (cause == RISCV_EXCP_U_ECALL) { cause = ecall_cause_map[env->priv]; 
...
s = set_field(s, MSTATUS_SPP, env->priv); 
s = set_field(s, MSTATUS_SIE, 0); 
env->mstatus = s;
...
env->pc = (env->stvec >> 2 << 2) + ((async && (env->stvec & 3) == 1) ? cause * 4 : 0);
...
riscv_cpu_set_mode(env, PRV_S);
...
cs->exception_index = EXCP_NONE;
```
从这些代码中，我们可以分别看到异常原因的判断、set_field设置寄存器的一些位（特别是SPP记录上一个特权级，SIE关中断）；env->pc记录了S态的入口地址，应该是要进入异常处理函数；riscv_cpu_set_mode切换特权级到S态；最后设置exception_index = EXCP_NONE表示异常处理结束，QEMU恢复CPU执行，这也解释了为什么我们在ucore输入si之后像是卡住了一样。
接下来就要找sret了，先在qemu调试终端把刚刚的断点删除，然后加上helper_sret的断点。下面就在ucore调试终端一直但不调试了。我们发现，下面的代码在trap.c中，然后被异常处理函数转发到内核的syscall(),并映射到了sys_fork()，这里面调用了do_fork()，是不是就很熟悉了？
在一切都执行完了之后，回到了trap.c中，调出了下面的结果：
```c
(gdb) n
trap (tf=<optimized out>)
    at kern/trap/trap.c:302
302             current->tf = otf;
(gdb) n
303             if (!in_kernel)
(gdb) n
305                 if (current->flags & PF_EXITING)
(gdb) n
309                 if (current->need_resched)
(gdb) n
__trapret ()
    at kern/trap/trapentry.S:131
131         RESTORE_ALL
(gdb) n
__trapret ()
    at kern/trap/trapentry.S:133
133         sret
```
的确出现了sret，但对于这个执行过程我没太搞明白。
### “我有点蒙，这代码是怎么调出来这个结果的？”
原来，我们为了加快调试，直接使用next而不是si，所以跳过了函数内部的具体代码，产生的效果就是进行了schedule，恢复了用户程序，接着上次触发异常的位置执行，这下是不是更体会到保存上下文的重要性了？
下面我们就可以去调qemu了，观察到helper_sret的关键代码:
```c
helper_sret (
    env=0x5a389a21a040, 
    cpu_pc_deb=18446744072637911122)
{...
target_ulong mstatus = env->mstatus;
prev_priv = get_field(mstatus, MSTATUS_SPP);
mstatus = set_field(mstatus, MSTATUS_SPIE, 0);
mstatus = set_field(mstatus, MSTATUS_SPP, PRV_U);
riscv_cpu_set_mode(env, prev_priv);
env->mstatus = mstatus;
return retpc;
}
```
我们发现这里读取了status中的重要信息，尤其是SPP位，还使用riscv_cpu_set_mode把特权级调整回去，最后返回spec中的内容，也就是用户态触发陷阱前的下一条指令地址。
### “TCG指令翻译是什么意思？helper_sret是在进行翻译吗？”
riscv_cpu_do_interrupt、helper_sret等函数都是用TCG对ecall、sret等指令进行翻译后，实现的处理逻辑。RISC-V 的 ecall/sret 是架构专属的特权指令，但我们是在 x86/ARM 主机上运行 QEMU 模拟 RISC-V，主机 CPU 不认识这些指令 —— 如果没有 TCG，QEMU 只能用 “解释执行”，也就是逐条解析指令含义再模拟，速度极慢。而 TCG 是 “动态翻译执行”，把 Guest 指令翻译成主机能执行的代码块，执行效率提升。在helper_sret执行之后看到的code_gen_buffer ()就是翻译后的代码缓存，也是提升效率的一种设计。
至于谁做了指令翻译，大模型说是tcg_gen_code()，我们也确实在这里打上了断点，函数里面的内容很多。在这个函数执行完毕之后，又调用了tb_gen_code ()，负责管理翻译块的生命周期。
想到在上一个分支任务中，查找TLB也用到了辅助函数helper_le_ldq_mmu，想必也经过了指令翻译成宿主机器码，才触发了辅助函数的调用。